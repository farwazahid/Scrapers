from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import re
import time

def website_scraper(url):
    options = Options()
    options.add_argument("start-maximized")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    options.add_argument('--incognito')
    driver = webdriver.Chrome(options=options)

    driver.get(url)
    time.sleep(5)  # Wait for initial page load

    # Scroll to the end of the page to load comments
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(3)  # Wait for potential additional content to load

    # Get page content and parse with BeautifulSoup
    page_content = driver.page_source
    soup = BeautifulSoup(page_content, 'html.parser')

    # Extract title, date, author, and editorial content
    title = soup.find('a', {'class': 'story_link'}).get_text(strip=True) if soup.find('a', {'class': 'story_link'}) else "Title not found"
    date = soup.find('span', {'class': 'story_time'}).get_text(strip=True) if soup.find('span', {'class': 'story_time'}) else "Date not found"
    author = soup.find('a', {'class': 'story_bylinelink'}).get_text(strip=True) if soup.find('a', {'class': 'storybyline_link'}) else "Author not found"
    editorial_content = "\n".join([p.get_text(strip=True) for p in soup.find_all('p')]) if soup.find_all('p') else "Editorial content not found"

    # Extract comment count using BeautifulSoup
    comment_count_div = soup.find('div', class_='font-playfair-display text-5 font-bold inline-block my-2')
    comment_count_text = comment_count_div.text if comment_count_div else "Comment count not available"
    if comment_count_div:
        # Use regular expressions to extract the number of comments
        match = re.search(r'\d+', comment_count_text)
        comment_count = match.group() if match else "0"
    else:
        comment_count = "Comment count not available"

    # Extract comments
    comments = [comment.text for comment in driver.find_elements(By.CLASS_NAME, 'comment__body')]

    driver.quit()

    data = {
        'URL': url,
        'Title': title,
        'Author': author,
        'Date': date,
        'Full Text': editorial_content,
        'Comment Count': comment_count,
        'Comments': comments
    }

    return data

# List of URLs to scrape
urls = ["https://www.dawn.com/news/1329909"]

# Initialize an empty list to store scraped data
data_list = []

# Scrape each URL
for url in urls:
    scraped_data = website_scraper(url)
    data_list.append(scraped_data)

# Create a DataFrame from the list of dictionaries
df = pd.DataFrame(data_list)

# Save to Excel
df.to_excel("Dawn.xlsx", index=False)
